[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.9.1","content-config-digest","35234a24d48863ab","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://johanwulf.github.io\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"experimentalDefaultStyles\":true},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"shiki\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,28,29,43,44],"building-shadecn",{"id":11,"data":13,"body":23,"filePath":24,"digest":25,"legacyId":26,"deferredRender":27},{"title":14,"date":15,"tags":16,"description":22},"Building shadecn - A shadcn theme-aware SVG tool",["Date","2025-06-11T00:00:00.000Z"],[17,18,19,20,21],"react","shadcn","svg","tools","typescript","How I built a tool to make SVGs work seamlessly with shadcn themes","I love using unDraw illustrations in my projects, but I'm way too lazy to manually change their colors to match my shadcn themes.\n\nSo I built [shadecn](https://github.com/johanwulf/shadecn) to fix exactly this problem (awesome name, right?).\n\n## The Problem\n\nWhen you export SVGs from Figma or other design tools, you get something like this:\n\n```jsx\n\u003Csvg fill=\"#8b5cf6\" stroke=\"#3b82f6\">\n  \u003Cpath d=\"...\" />\n\u003C/svg>\n```\n\nThose hex colors are baked in. They don't respond to theme changes, and they probably don't match your design system anyway.\n\n## The Solution\n\nshadecn lets you import your shadcn theme CSS and map those hard-coded colors to your theme variables. Instead of `fill=\"#8b5cf6\"`, you get `fill=\"hsl(var(--primary))\"`.\n\nHere's how it works:\n\n1. Paste your SVG\n2. Import your shadcn theme config (the CSS with all those OKLCH colors)\n3. Map which SVG colors should use which theme colors\n4. Export a React component that adapts to your theme\n\n## Tech Stack\n\nBuilt with React, TypeScript, and of course shadcn/ui components. Using Tailwind CSS for styling and Vite for the build process. The color conversion magic happens with culori.\n\nIt's live at [wulf.gg/shadecn](https://wulf.gg/shadecn) and the code is on [GitHub](https://github.com/johanwulf/shadecn).\n\n## Why I Built This\n\nHonestly, I just got tired of manually editing SVG colors every time I wanted to use an illustration in a project. Now I can paste an SVG, map it to my theme, and get a component that just works everywhere.","src/content/blog/building-shadecn.mdx","6d77f043b50f99b1","building-shadecn.mdx",true,"migrating-to-astro",{"id":28,"data":30,"body":39,"filePath":40,"digest":41,"legacyId":42,"deferredRender":27},{"title":31,"date":32,"tags":33,"description":38},"Migrating to Astro",["Date","2025-06-09T00:00:00.000Z"],[34,35,36,37],"astro","web","migration","mdx","How I migrated my personal website from Vite + React to Astro","After struggling with complex MDX configurations and build tool conflicts, I decided to migrate my personal website to Astro.\n\n## The Problem\n\nI started with a Vite + React + MDX setup, which seemed like a good idea at first. But as I added more features, the complexity grew:\n\n- PostCSS and Tailwind CSS v4 conflicts\n- Complex MDX configuration with multiple plugins\n- Manual routing for blog posts\n- Heavy JavaScript bundle for a mostly static site\n\n## Enter Astro\n\nAstro is a modern static site generator that's perfect for content-focused websites. Here's what sold me:\n\n### Zero JavaScript by Default\n\nUnlike traditional React apps, Astro ships zero JavaScript by default. Your pages are fully rendered at build time.\n\n### Built-in MDX Support\n\n```js\n// astro.config.mjs\nimport mdx from '@astrojs/mdx'\n\nexport default defineConfig({\n  integrations: [mdx()]\n})\n```\n\n### Content Collections\n\nAstro's content collections provide type-safe content management:\n\n```typescript\nimport { defineCollection, z } from 'astro:content'\n\nconst blogCollection = defineCollection({\n  type: 'content',\n  schema: z.object({\n    title: z.string(),\n    date: z.coerce.date(),\n    tags: z.array(z.string()),\n    description: z.string().optional(),\n  }),\n})\n```\n\n## The Migration Process\n\nThe migration was surprisingly smooth:\n\n1. **Created Astro project structure**\n   - Pages go in `src/pages/`\n   - Layouts in `src/layouts/`\n   - Blog posts in `src/content/blog/`\n\n2. **Converted React components to Astro**\n   - Most components became `.astro` files\n   - Kept React only for interactive components\n\n## The Results\n\nThe page you are looking at!\n\n## Code Example\n\nHere's how simple a blog post page is in Astro:\n\n```astro\n---\nimport BaseLayout from '../../layouts/BaseLayout.astro'\nimport { getCollection } from 'astro:content'\n\nexport async function getStaticPaths() {\n  const posts = await getCollection('blog')\n  return posts.map(post => ({\n    params: { slug: post.slug },\n    props: { post },\n  }))\n}\n\nconst { post } = Astro.props\nconst { Content } = await post.render()\n---\n\n\u003CBaseLayout title={post.data.title}>\n  \u003Carticle>\n    \u003Ch1>{post.data.title}\u003C/h1>\n    \u003CContent />\n  \u003C/article>\n\u003C/BaseLayout>\n```","src/content/blog/migrating-to-astro.mdx","cabd4ca49560add8","migrating-to-astro.mdx","ci-cd-cost-spike",{"id":43,"data":45,"body":55,"filePath":56,"digest":57,"legacyId":58,"deferredRender":27},{"title":46,"date":47,"tags":48,"description":54},"How I Increased Our Cloud Costs by 16,000% With a CI/CD \"Improvement\"",["Date","2025-06-13T00:00:00.000Z"],[49,50,51,52,53],"ci/cd","github-actions","docker","gcp","testing","A cautionary tale about parallelizing E2E tests and the hidden costs of Docker image pulls","Last week, I turned our $0.50/day GCP Artifact Registry into an $80/day money pit. Here's how a simple CI/CD improvement went horribly wrong.\n\n## The Problem\n\nOur E2E tests were running after merge to main, which meant broken code was making it to production. The obvious fix: run them on pull requests instead.\n\nBut there was a catch - running all Cypress tests sequentially took over 20 minutes. No developer wants to wait that long for PR checks. So parallelization was a requirement from the start.\n\n## The Solution (That Wasn't)\n\nMy approach seemed reasonable: use GitHub Actions' matrix strategy to run each test file in parallel.\n\n```yaml\nfind-test-files:\n  runs-on: ubuntu-latest\n  outputs:\n    test-files: ${{ steps.find.outputs.files }}\n  steps:\n    - name: Find test files\n      id: find\n      run: |\n        # Find all test files and output as JSON array\n        echo \"files=$(find e2e -name \"*.spec.js\" | jq -R . | jq -sc '.')\" >> $GITHUB_OUTPUT\n\nrun-tests:\n  strategy:\n    fail-fast: false\n    matrix:\n      test-file: ${{ fromJson(needs.find-test-files.outputs.test-files) }}\n```\n\nThis spawned about 30 parallel runners, each pulling our Docker images:\n\n```yaml\n- name: Authenticate to registry\n  uses: google-github-actions/auth@v2\n  with:\n    workload_identity_provider: ${{ env.WIP }}\n    service_account: ${{ env.SERVICE_ACCOUNT }}\n\n- name: Pull Docker images\n  run: |\n    # Authenticate to artifact registry\n    docker login -u oauth2accesstoken -p ${{ steps.auth.outputs.access_token }} $REGISTRY_URL\n    \n    # Pull all required images\n    docker pull $REGISTRY_URL/database:latest\n    docker pull $REGISTRY_URL/backend-api:latest\n    docker pull $REGISTRY_URL/services:latest\n    docker pull $REGISTRY_URL/frontend:latest\n```\n\nEach runner needed:\n- A PostgreSQL database image\n- Our backend API image\n- Our services image\n- The frontend orchestrator image\n\nTests ran fast. PRs were protected. I shipped it.\n\n## The Wake-Up Call\n\nNext morning, my manager sends me a screenshot of our GCP billing dashboard with just \"???\"\n\nThe graph showed a massive spike. Our Artifact Registry costs had jumped from $0.50 to $80 per day. That's a 16,000% increase.\n\nEvery single runner was pulling the full Docker images independently. With 30 runners and multiple large images, we were transferring the same data 30 times per test run.\n\n## The Fix\n\nThe solution was embarrassingly straightforward: pull images once, cache them, then load from cache in each runner.\n\n```yaml\nprepare-images:\n  runs-on: ubuntu-latest\n  outputs:\n    cache-key: ${{ steps.cache-key.outputs.key }}\n  steps:\n    - name: Generate cache key from image digests\n      id: cache-key\n      run: |\n        # Get digest for each image\n        DIGESTS=\"\"\n        for IMAGE in $IMAGE_LIST; do\n          DIGEST=$(docker manifest inspect $IMAGE | jq -r '.config.digest')\n          DIGESTS=\"${DIGESTS}-${DIGEST}\"\n        done\n        echo \"key=docker-images-${DIGESTS}\" >> $GITHUB_OUTPUT\n\n    - name: Cache Docker images\n      id: cache\n      uses: actions/cache@v4\n      with:\n        path: /tmp/docker-images\n        key: ${{ steps.cache-key.outputs.key }}\n\n    - name: Pull and save images\n      if: steps.cache.outputs.cache-hit != 'true'\n      run: |\n        mkdir -p /tmp/docker-images\n        for IMAGE in $IMAGE_LIST; do\n          docker pull $IMAGE\n          docker save $IMAGE -o /tmp/docker-images/$(basename $IMAGE).tar\n        done\n```\n\nThen in each test runner:\n\n```yaml\n- name: Restore Docker images from cache\n  uses: actions/cache@v4\n  with:\n    path: /tmp/docker-images\n    key: ${{ needs.prepare-images.outputs.cache-key }}\n    fail-on-cache-miss: true\n\n- name: Load all images in parallel\n  run: |\n    for IMAGE_TAR in /tmp/docker-images/*.tar; do\n      docker load -i $IMAGE_TAR &\n    done\n    wait\n```\n\nThe key insight: use Docker image digests as cache keys. This ensures we only pull when images actually change.\n\n## Lessons Learned\n\n1. **Think about shared resources when parallelizing**. What works for one runner might be wasteful for 30.\n\n2. **Monitor your cloud costs daily**. Set up billing alerts. Don't wait for your manager to notice.\n\n3. **Cache aggressively in CI/CD**. Docker images, dependencies, build artifacts - if you're pulling it more than once, you're probably doing it wrong.\n\n## The Numbers\n\n- **Before**: $0.50/day\n- **During the incident**: $80/day\n- **After the fix**: less than $1/day\n- **Parallel runners**: ~30 test files\n- **Test duration**: Reduced from 20+ minutes to ~5 minutes with parallelization\n\n## The Current Solution\n\nOur E2E tests now run efficiently with proper caching. The prepare job pulls images once, caches them with digest-based keys, and all test runners load from cache. We're back to reasonable costs while maintaining fast test execution.\n\n```yaml\n# Prevent multiple runs for the same PR\nconcurrency:\n  group: e2e-pr-${{ github.event.pull_request.number }}\n  cancel-in-progress: true\n```\n\nWe also added concurrency groups to prevent multiple test runs from the same PR, further reducing unnecessary pulls.\n\n## Takeaway\n\nParallelization is powerful, but it amplifies both efficiency and inefficiency. Always consider the cost implications of your architectural decisions, especially when dealing with cloud resources.","src/content/blog/ci-cd-cost-spike.mdx","f9cb5ffa4b36cbb3","ci-cd-cost-spike.mdx"]